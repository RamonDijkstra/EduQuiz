{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ced4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76867e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables to perform evalution for you task\n",
    "original_quiz_path = os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'processed_data/gpt3/completion_4/processed_test.json'\n",
    "question_path = os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/SWQG/generated_questions.json'\n",
    "answer_path = os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/SWQG/generated_answers.json'\n",
    "distractor_path = os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/SWQG/generated_distractors.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b66793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original quizzes\n",
    "original_quizzes = []\n",
    "\n",
    "for line in open(original_quiz_path):\n",
    "    original_quizzes.append(json.loads(line)['completion'].split(\"\\n###\")[0].replace(\"\\n\",\" \").strip())\n",
    "    \n",
    "# Get generated questions\n",
    "questions = []\n",
    "\n",
    "with open(question_path) as f:\n",
    "    generated = json.load(f)\n",
    "\n",
    "for key in generated:\n",
    "    questions.append(generated[key])\n",
    "    \n",
    "# Get generated answers\n",
    "answers = []\n",
    "\n",
    "with open(answer_path) as f:\n",
    "    generated = json.load(f)\n",
    "\n",
    "for key in generated:\n",
    "    answers.append(generated[key])\n",
    "    \n",
    "# Get generated distractors\n",
    "distractors = []\n",
    "\n",
    "with open(distractor_path) as f:\n",
    "    generated = json.load(f)\n",
    "\n",
    "for key in generated:\n",
    "    distractors.append(generated[key])\n",
    "\n",
    "generated_quizzes = []\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    quiz = questions[i].strip() + \" True answer: \" + answers[i].split(\"Answer: \")[1].strip() + \" \" + distractors[i].replace(\"\\n\",\" \").strip()\n",
    "    generated_quizzes.append(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9dfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the predictions and gold references in lists\n",
    "\n",
    "predictions = []\n",
    "gold_references = []\n",
    "predictions_list = []\n",
    "gold_references_list = []\n",
    "\n",
    "for i in range(len(original_quizzes)):\n",
    "    predictions.append(generated_quizzes[i])\n",
    "    gold_references.append(original_quizzes[i])\n",
    "\n",
    "    predictions_list.append(generated_quizzes[i].split(' '))\n",
    "    gold_references_list.append([original_quizzes[i].split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7514ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "bleu = datasets.load_metric('bleu')\n",
    "rouge = datasets.load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957df6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu.add_batch(predictions=predictions_list, references=gold_references_list)\n",
    "rouge.add_batch(predictions=predictions, references=gold_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc14283",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bleu = bleu.compute()\n",
    "final_rouge = rouge.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd7208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of downloaded meteor from https://www.cs.cmu.edu/~alavie/METEOR/\n",
    "meteor_path = \"<PATH_TO_YOUR_DOWNLOADED_METEOR>\"\n",
    "\n",
    "# Move results to meteor directory\n",
    "with open(meteor_path + \"predictions.txt\", 'w') as f:\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(str(predictions[i]) + '\\n')\n",
    "        \n",
    "with open(meteor_path + \"ground_truth.txt\", 'w') as f:\n",
    "    for i in range(len(gold_references)):\n",
    "        f.write(str(gold_references[i]) + '\\n')\n",
    "\n",
    "# Run the meteor command from the meteor directory and remove result files again   \n",
    "wd = os.getcwd()\n",
    "os.chdir(meteor_path)\n",
    "output = os.popen(\"java -Xmx2G -jar meteor-*.jar predictions.txt ground_truth.txt -l en -norm\").read()\n",
    "os.remove(meteor_path + \"predictions.txt\")\n",
    "os.remove(meteor_path + \"ground_truth.txt\")\n",
    "os.chdir(wd)\n",
    "\n",
    "# Get the score from the output\n",
    "meteor_score = round(float(output.split(\"Final score:\")[1].strip()) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f4a662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU:  9.67\n",
      "ROUGE-L:  34.92\n",
      "METEOR:  24.88\n"
     ]
    }
   ],
   "source": [
    "print(\"BLEU: \", str(round(final_bleu['bleu'] * 100, 2)))\n",
    "print(\"ROUGE-L: \", str(round(final_rouge['rougeL'].mid.fmeasure * 100, 2)))\n",
    "print(\"METEOR: \", str(meteor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "331f30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result files\n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/SWQG/' + 'automatic_evaluation_SWQG.txt', 'w') as f:\n",
    "    f.write(\"BLEU: \" + str(round(final_bleu['bleu'] * 100, 2)))\n",
    "    f.write('\\n')\n",
    "    f.write(\"ROUGE-L: \" + str(round(final_rouge['rougeL'].mid.fmeasure * 100, 2)))\n",
    "    f.write('\\n')\n",
    "    f.write(\"METEOR: \" + str(meteor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc0fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the target prompts\n",
    "test_data = []\n",
    "\n",
    "for line in open(original_quiz_path):\n",
    "    test_data.append((json.loads(line)))\n",
    "\n",
    "count = 0\n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'human_evaluation/SWQG/' + 'SWQG_gpt3.txt', 'w') as f:\n",
    "    for i in range(0,len(test_data),math.floor(len(test_data)/100)):\n",
    "        f.write('Excel row: ' + str(count+2) + ' Test instance: ' + str(i+1) + '\\n\\n')\n",
    "        f.write(test_data[i]['prompt'].split(\"\\n\\n###\")[0] + '\\n\\n')\n",
    "        f.write('Generated quiz:\\n')\n",
    "        f.write(questions[i].strip() + '\\n')\n",
    "        f.write(answers[i].strip() + '\\n')\n",
    "        f.write(distractors[i].strip() + '\\n\\n')\n",
    "        f.write('----------------------------------------------------------------------------------------' + '\\n\\n')\n",
    "        count+=1\n",
    "        \n",
    "        if count == 100:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
